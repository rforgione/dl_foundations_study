{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# default_exp feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.datasets import *\n",
    "import pathlib\n",
    "import gzip\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "MNIST_URL = \"http://deeplearning.net/data/mnist/mnist.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_path = download_data(MNIST_URL, ext='.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/paperspace/.fastai/data/mnist.pkl.gz')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with gzip.open(data_path, \"rb\") as f: \n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    (x_train, y_train, x_valid, y_valid) = map(torch.tensor, (x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 784])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "So we have the flattened pixels from 60k 28x28 images, giving us 60k x 784 elements. We break that into a training and a validation set of 50k and 10k examples, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Basic Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Here's our first basic stab at matrix multiplication. It's going to be very slow, because we're doing the whole thing in python. It's nice for understanding exactly what we're working with, but we'll see very soon that there are some tricks we can use to speed this up a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def basic_matmul(a, b):\n",
    "    assert a.shape[1] == b.shape[0]\n",
    "    output = torch.zeros(a.shape[0], b.shape[1]).float()\n",
    "    for i in range(a.shape[0]):\n",
    "        for j in range(b.shape[1]):\n",
    "            for k in range(a.shape[1]):\n",
    "                output[i,j] += a[i,k] * b[k,j]\n",
    "    return output                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "mat1 = torch.tensor([[1,2],\n",
    "                     [3,4]])\n",
    "\n",
    "mat2 = torch.tensor([[5,6], \n",
    "                     [7,8], \n",
    "                     [9,10]])\n",
    "try:\n",
    "    basic_matmul(mat1, mat2)\n",
    "    raise ValueError # if it doesn't throw an assertion error, we get here\n",
    "except AssertionError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "mat3 = torch.tensor([[5,6], \n",
    "                     [7,8]])\n",
    "expected = torch.tensor([\n",
    "    [19,22],\n",
    "    [43,50]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 22.],\n",
       "        [43., 50.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_matmul(mat1, mat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def allclose(a, b, tol=1e-3): return (a - b).max() < tol    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "assert allclose(basic_matmul(mat1, mat3), expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "NUM_HIDDEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "weights = torch.randn(x_train.shape[1], NUM_HIDDEN)\n",
    "biases = torch.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 924 ms, sys: 0 ns, total: 924 ms\n",
      "Wall time: 925 ms\n"
     ]
    }
   ],
   "source": [
    "%time t1=basic_matmul(x_train[:5], weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Ok, so this is terribly slow. It takes us 924ms to do the matrix multiplication for 5 rows of x_train with a hidden layer size of 10. This data is super small, and yet it still takes almost a full second. That's definitely not gonna get it done! Alas, we need to speed things along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# PyTorch Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "First things first, we can use pytorch's built-in operations to get rid of the innermost loop. Often when numerical operations are implemented in base pytorch, they're implemented in aten, which makes them super fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def dot_prod_matmul(a, b):\n",
    "    assert a.shape[1] == b.shape[0]\n",
    "    output = torch.zeros(a.shape[0], b.shape[1]).float()\n",
    "    for i in range(a.shape[0]):\n",
    "        for j in range(b.shape[1]):\n",
    "            output[i,j] = (a[i,:] * b[:,j]).sum()\n",
    "    return output                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "assert allclose(dot_prod_matmul(mat1, mat3), expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 2.53 ms\n"
     ]
    }
   ],
   "source": [
    "%time t1=dot_prod_matmul(x_train[:5], weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Wow! So we're down from 924ms to 2.53ms. That means the basic version is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365.21739130434787"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "924/2.53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "...365 times slower than the raw pytorch version. Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The reason the code above is so slow is because all of the work is being done in actual python. Python itself is super slow. Any highly performant operations that run in python typically delegate to a faster language. In the case of pytorch, the operation is delegated to a low-level, highly performant language called aten. Vectorized operations are delegated to aten, which speeds them up.\n",
    "\n",
    "\n",
    "Here are the rules of broadcasting ([source](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html)):\n",
    "* Rule 1: If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded with ones on its leading (left) side.\n",
    "* Rule 2: If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape.\n",
    "* Rule 3: If in any dimension the sizes disagree and neither is equal to 1, an error is raised.\n",
    "\n",
    "Here are some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "a = 1; b = torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In this case, the scalar is converted to a 0-dim vector, and then an axis of length 1 is prepended onto it making it a 1-dim vector of length 1. Then, it is stretched along that vector to match the size of of the second vector, and the two are added together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "a = torch.tensor(\n",
    "    [[1],\n",
    "     [2],\n",
    "     [3]]\n",
    ")\n",
    "b = torch.tensor([[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 4],\n",
       "        [3, 4, 5],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In this case, broadcasting happens in both directions. First we look at the vertical dimension. `a` has size 3 while `b` has size 1, so `b` is repeated three times to make them match. Then we look at the horizontal dimension. `b` has size 3 but `a` has size 1, so `a` is repeated three times to match. Then we add them together element wise to get our final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We can use broadcasting to remove the innermost loop of our matmul function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
